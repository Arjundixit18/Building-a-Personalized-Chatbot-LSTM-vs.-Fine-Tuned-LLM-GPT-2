{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb09163-da15-4b00-a4a2-cf3acec82a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c7d41-42a0-40c9-b9e0-2a8f5f50e28b",
   "metadata": {},
   "source": [
    "# FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44dfb3ab-5579-4547-918f-949691d144d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eace1ac576c4b8aa42742e3e6cdd72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Windows\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee00187719a645bd8677e77dbf76ba4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecff5d18c6a446298aca89d1aa973ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a574bb40e968472981ba55e71a0e2e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127fe7b8add941bfb9e456c71a6b6b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"gpt2\"  # or \"gpt2-medium\" for 355M\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b672a3-6e58-4823-a06a-4e0f8b318ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=128\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff78c9c9-aae7-4c9c-99c4-df7925938052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\anaconda3\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (276626 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"chat.txt\", tokenizer)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bad0d63-53e0-43a8-8085-417bc636b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fae66b90018444d9b40165ec9377e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185af4a769f34f3c93cdec0b0353858e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c6c835b-db26-4dfa-a07e-7aadc2f06572",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    fp16=False,  # No GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7082b6e-ffc6-4953-b55a-8b7f7349370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5022da1-b643-40a9-bb7b-b728b80a4741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3243' max='3243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3243/3243 5:28:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.916200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.563500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.390900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.380200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.373300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.382400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.229100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.279900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.166100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.129200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-friend-model\\\\tokenizer_config.json',\n",
       " './gpt2-friend-model\\\\special_tokens_map.json',\n",
       " './gpt2-friend-model\\\\vocab.json',\n",
       " './gpt2-friend-model\\\\merges.txt',\n",
       " './gpt2-friend-model\\\\added_tokens.json',\n",
       " './gpt2-friend-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./gpt2-friend-model\")\n",
    "tokenizer.save_pretrained(\"./gpt2-friend-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c47eed6-15bf-4dfc-8f71-8af43a34f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me: ki korchis re? \n",
      "Her: korchis\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"./gpt2-friend-model\", tokenizer=\"./gpt2-friend-model\")\n",
    "prompt = \"Me: ki korchis re? \\nHer:\"\n",
    "\n",
    "\n",
    "response = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=50256,\n",
    ")\n",
    "\n",
    "generated_text = response[0]['generated_text']\n",
    "\n",
    "def clean_output(text):\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        if re.match(r'^\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2} - ', line):\n",
    "            continue \n",
    "        if '<Media omitted>' in line:\n",
    "            continue \n",
    "        cleaned.append(line)\n",
    "    return \"\\n\".join(cleaned)\n",
    "\n",
    "print(clean_output(generated_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d6fee575-f9bf-40ab-8d95-fad1d96de25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# generator = pipeline('text-generation', model=\"./gpt2-friend-model\", tokenizer=\"./gpt2-friend-model\")\n",
    "\n",
    "# prompt = \"A funny and slightly sarcastic Bengali girl is chatting with her best friend using English letters.\\nFriend: AU Sir ki poralo re?\\nHer:\"\n",
    "# response = generator(prompt, max_new_tokens=50, do_sample=True, temperature=0.7)\n",
    "# print(response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fcc736-49fb-4073-b2ed-06d6b1164c39",
   "metadata": {},
   "source": [
    "# LSTM from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e186331-72c4-4220-8fb7-cd6cd03546d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat converted and saved to 'new_chat.txt'\n"
     ]
    }
   ],
   "source": [
    "# transforming chat.txt dataset\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_file = \"chat.txt\"       # your original chat file\n",
    "output_file = \"new_chat.txt\"      # the output file name\n",
    "\n",
    "# === NAME MAPPING ===\n",
    "speaker_map = {\n",
    "    \"Sneha Dutta AEC CSE\": \"Her\",\n",
    "    \"Debkumar Baksi\": \"Me\"\n",
    "}\n",
    "\n",
    "# === PATTERN TO MATCH WHATSAPP MESSAGES ===\n",
    "message_pattern = re.compile(r'^(\\d{2}/\\d{2}/\\d{4}), \\d{2}:\\d{2} - ([^:]+): (.*)')\n",
    "\n",
    "# === PROCESSING ===\n",
    "processed_lines = []\n",
    "current_speaker = None\n",
    "current_message = []\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"<media omitted>\" in line.lower():\n",
    "            continue\n",
    "        match = message_pattern.match(line)\n",
    "        if match:\n",
    "            # Save previous message\n",
    "            if current_speaker and current_message:\n",
    "                speaker_label = speaker_map.get(current_speaker, current_speaker)\n",
    "                processed_lines.append(f\"{speaker_label}: {' '.join(current_message)}\")\n",
    "            # Start new message\n",
    "            current_speaker = match.group(2)\n",
    "            current_message = [match.group(3)]\n",
    "        else:\n",
    "            # Continuation of the previous message\n",
    "            if current_message is not None:\n",
    "                current_message.append(line)\n",
    "\n",
    "# Add the last message\n",
    "if current_speaker and current_message:\n",
    "    speaker_label = speaker_map.get(current_speaker, current_speaker)\n",
    "    processed_lines.append(f\"{speaker_label}: {' '.join(current_message)}\")\n",
    "\n",
    "# === SAVE TO FILE ===\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(processed_lines))\n",
    "\n",
    "print(f\"Chat converted and saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd620714-6b50-4768-b30e-017236c14606",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0c5baa7-b537-4e7f-884d-f5f6703fd978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 216\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned WhatsApp chat\n",
    "with open(\"new_chat.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Character mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char2idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Print info\n",
    "print(f\"Unique characters: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b927415-4086-45a6-a6ec-75fd8ddbf526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, text, seq_len=100):\n",
    "        self.data = text\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq = self.data[idx : idx + self.seq_len]\n",
    "        y_seq = self.data[idx + 1 : idx + self.seq_len + 1]\n",
    "        x = torch.tensor([char2idx[ch] for ch in x_seq], dtype=torch.long)\n",
    "        y = torch.tensor([char2idx[ch] for ch in y_seq], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "708cdec7-6ec4-4e5e-b9d2-46390cf4a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2d39ec1-e669-4fc3-8788-9716cf7c4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m---> 33\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1295\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1297\u001b[0m         target,\n\u001b[0;32m   1298\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1299\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[0;32m   1300\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1301\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[0;32m   1302\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[0;32m   3495\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3496\u001b[0m     target,\n\u001b[0;32m   3497\u001b[0m     weight,\n\u001b[0;32m   3498\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[0;32m   3499\u001b[0m     ignore_index,\n\u001b[0;32m   3500\u001b[0m     label_smoothing,\n\u001b[0;32m   3501\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "seq_len = 50\n",
    "batch_size = 32\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "epochs = 5\n",
    "lr = 0.003\n",
    "\n",
    "dataset = ChatDataset(text, seq_len)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "model = CharLSTM(vocab_size, hidden_size, num_layers).to(\"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "print(\"üîÅ Starting training...\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "        out, _ = model(x)\n",
    "        loss = loss_fn(out.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Epoch {epoch+1} complete - Loss: {total_loss / len(loader):.4f} - Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ Training finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6420a3d-c46a-4466-bb15-4f3b5bf5feb3",
   "metadata": {},
   "source": [
    "## save model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a02c0751-f922-4072-b1c7-ca4efcf6486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved as chat_lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model and mappings\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'char2idx': char2idx,\n",
    "    'idx2char': idx2char,\n",
    "    'vocab_size': vocab_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_layers': num_layers\n",
    "}, \"chat_lstm_model.pth\")\n",
    "\n",
    "print(\"‚úÖ Model saved as chat_lstm_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abcd43-c063-4008-994f-d6b918470e7c",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e30d187-9992-4148-9912-f7656c4de11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"chat_lstm_model.pth\")\n",
    "\n",
    "# Rebuild the mappings and model\n",
    "char2idx = checkpoint['char2idx']\n",
    "idx2char = checkpoint['idx2char']\n",
    "vocab_size = checkpoint['vocab_size']\n",
    "hidden_size = checkpoint['hidden_size']\n",
    "num_layers = checkpoint['num_layers']\n",
    "\n",
    "# Rebuild and load the model\n",
    "model = CharLSTM(vocab_size, hidden_size, num_layers)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e17e1443-8f13-4894-b7c1-0805ec95365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(prompt, length=300):\n",
    "#     model.eval()\n",
    "#     input_idx = torch.tensor([char2idx.get(ch, 0) for ch in prompt], dtype=torch.long).unsqueeze(0)\n",
    "#     generated = list(prompt)\n",
    "#     hidden = None\n",
    "\n",
    "#     for _ in range(length):\n",
    "#         with torch.no_grad():\n",
    "#             output, hidden = model(input_idx[:, -1:], hidden)\n",
    "#         next_id = torch.argmax(output[0, -1]).item()\n",
    "#         next_char = idx2char[next_id]\n",
    "#         generated.append(next_char)\n",
    "#         input_idx = torch.cat([input_idx, torch.tensor([[next_id]])], dim=1)\n",
    "\n",
    "#     return ''.join(generated)\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_next(output, temperature=1.0):\n",
    "    logits = output[0, -1] / temperature\n",
    "    probs = F.softmax(logits, dim=0)\n",
    "    return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "def generate_text(prompt, length=300, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_idx = torch.tensor([char2idx.get(ch, 0) for ch in prompt], dtype=torch.long).unsqueeze(0)\n",
    "    generated = list(prompt)\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_idx[:, -1:], hidden)\n",
    "        next_id = sample_next(output, temperature)\n",
    "        next_char = idx2char[next_id]\n",
    "        generated.append(next_char)\n",
    "        input_idx = torch.cat([input_idx, torch.tensor([[next_id]])], dim=1)\n",
    "\n",
    "    return ''.join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe33c4ee-a344-418e-88ac-cc28644a20b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me:ki korchhis re?\n",
      "Her: üôÇüôÇüôÇüôÇbyoum\n",
      "Her: amdr\n",
      "Me: Mondr kore ni\n",
      "Me: upch ker 6e n\n",
      "Her: kal lab lag6er\n",
      "Herr: saruu\n",
      "Her: Kame jbo onek diye\n",
      "Me: Oito hoye6lin?\n",
      "Her: Amke\n",
      "Her: Miss\n",
      "Me: Ami aubha\n",
      "Her: poc korechhis?\n",
      "Me: Ei nijekh\n",
      "Me: ü§£ü§£ü§£ü§£...j to join oble\n",
      "Her: au kisr\n",
      "Her: Bhai to jbi kalke e krbe softa ta\n",
      "Me: indic e to bollo\n",
      "M\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Me:ki korchhis re?\\nHer:\"\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9c98d-035b-4004-8db9-e257ac0a487d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
