
# ğŸ§  Chat with My Best Friend â€” Custom Chatbot Using LSTM and GPT-2

A personalized chatbot that mimics my best friend's style using exported WhatsApp chats.  
Two parallel approaches explored:
- A character-level **LSTM** model (built from scratch using PyTorch)
- A fine-tuned **GPT-2** model (using Hugging Face Transformers)

## ğŸ“Œ Project Highlights

- Dataset: 250KB of exported WhatsApp chat in transliterated Bengali (`tui kemon achhis?`)
- Preprocessing: Cleaned timestamps, formatted dialogues, removed media tags
- Training:
  - LSTM: 2-layer, 64â€“128 hidden units, ~30 mins CPU training
  - GPT-2: Fine-tuned on full dataset, ~5 hours CPU training
- Generation: Prompt-based responses using character decoding (LSTM) and BPE sampling (GPT-2)

## ğŸ“– Read the Technical Comparison Article

ğŸ‘‰ [**LSTM vs GPT-2 for Personalized Chatbots** on LinkedIn](https://www.linkedin.com/posts/debkumar-baksi-269738279_machinelearning-nlp-llm-activity-7347277817748836352-ulfv?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAEPrDnEBVjAz3BltRhVC2Ye2Hln8TZTZw70)  


## ğŸ› ï¸ Tech Stack

| Component      | Details                         |
|----------------|----------------------------------|
| Framework      | PyTorch, Hugging Face Transformers |
| Model 1        | Custom Char-Level LSTM           |
| Model 2        | GPT-2 (117M parameters)           |
| Training       | CPU-only, ~30â€“300 mins           |
| Language Style | Bengali in Roman script          |
| Input Format   | `Me: ...` + `Her: ...`           |


## ğŸ§ª Example Output

### Prompt
```
Me: Ki korchhis re?
```


### LSTM Output
```
Her: ğŸ™‚ğŸ™‚ğŸ™‚ğŸ™‚byoum
Her: amdr
Me: Mondr kore ni
Me: upch ker 6e n
Her: kal lab lag6er
Herr: saruu
Her: Kame jbo onek diye
Me: Oito hoye6lin?
Her: Amke
Her: Miss
Me: Ami aubha
Her: poc korechhis?
Me: Ei nijekh
Me: ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£...j to join oble
Her: au kisr
Her: Bhai to jbi kalke e krbe softa ta
Me: indic e to bollo
M
```


### GPT-2 Output
```
Her: korchis
```

## ğŸš€ How to Run (LSTM Version)

1. Clone the repo:
   ```bash
   https://github.com/Debkumar-Baksi/Building-a-Personalized-Chatbot-LSTM-vs.-Fine-Tuned-LLM-GPT-2.git
   cd Building-a-Personalized-Chatbot-LSTM-vs.-Fine-Tuned-LLM-GPT-2
2. Install requirements:

```
pip install torch tqdm
```
3. Run:
```
python clone_friend.ipynb
```
## ğŸ’¡ Future Work
Word-level LSTM with embeddings

Fine-tune multilingual LLMs (e.g., IndicBERT, BanglaGPT)

Deploy using Gradio

Quantized GPT-2 for mobile use

## ğŸ¤ Author
Debkumar Baksi
BTech CSE | Python Developer | AI Enthusiast
LinkedIn
