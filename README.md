# ğŸ§  Chat with My Best Friend â€” Custom Chatbot Using LSTM and GPT-2

A personalized chatbot that mimics my best friend's style using exported WhatsApp chats.  
Two parallel approaches explored:
- A character-level **LSTM** model (built from scratch using PyTorch)
- A fine-tuned **GPT-2** model (using Hugging Face Transformers)

## ğŸ“Œ Project Highlights

- Dataset: 250KB of exported WhatsApp chat in transliterated Bengali (`tui kemon achhis?`)
- Preprocessing: Cleaned timestamps, formatted dialogues, removed media tags
- Training:
  - LSTM: 2-layer, 64â€“128 hidden units, ~30 mins CPU training
  - GPT-2: Fine-tuned on full dataset, ~5 hours CPU training
- Generation: Prompt-based responses using character decoding (LSTM) and BPE sampling (GPT-2)

## ğŸ“– Read the Technical Comparison Article

ğŸ‘‰ [**LSTM vs GPT-2 for Personalized Chatbots** on LinkedIn](https://www.linkedin.com/in/debkumar-baksi)  
(*Replace the link with the actual article URL when published*)

## ğŸ› ï¸ Tech Stack

| Component      | Details                         |
|----------------|----------------------------------|
| Framework      | PyTorch, Hugging Face Transformers |
| Model 1        | Custom Char-Level LSTM           |
| Model 2        | GPT-2 (117M parameters)           |
| Training       | CPU-only, ~30â€“300 mins           |
| Language Style | Bengali in Roman script          |
| Input Format   | `Me: ...` + `Her: ...`           |

## ğŸ“‚ Project Structure

â”œâ”€â”€ data/
â”‚ â””â”€â”€ whatsapp_chat.txt # Raw exported chat
â”‚ â””â”€â”€ new_chat_data.txt # Cleaned & formatted
â”œâ”€â”€ lstm_model.py # Char-level LSTM model
â”œâ”€â”€ train_lstm.py # LSTM training script
â”œâ”€â”€ generate_lstm.py # LSTM-based generator
â”œâ”€â”€ gpt2_finetune.py # GPT-2 fine-tuning script
â”œâ”€â”€ chat_lstm_model.pth # Saved LSTM weights
â”œâ”€â”€ README.md


## ğŸ§ª Example Output

### Prompt
```
Me: tumi porcho?
Her:
```


### LSTM Output
```
Her: ki krbo
Her: ki krbo
Her: ki krbo
```


### GPT-2 Output
```
Her: ha porchi. Tui bol ki korchish?
```

## ğŸš€ How to Run (LSTM Version)

1. Clone the repo:
   ```bash
   git clone https://github.com/yourusername/chat-with-best-friend.git
   cd chat-with-best-friend
2. Install requirements:

```
pip install torch tqdm
```
3. Train:
```
python train_lstm.py
```
4. Generate:
```
python generate_lstm.py
```
## ğŸ’¡ Future Work
Word-level LSTM with embeddings

Fine-tune multilingual LLMs (e.g., IndicBERT, BanglaGPT)

Deploy using Gradio

Quantized GPT-2 for mobile use

## ğŸ¤ Author
Debkumar Baksi
BTech CSE | Python Developer | AI Enthusiast
LinkedIn